#!/bin/bash

PROJECT_ROOT_FROM_SCRIPTS_LOCATION=.

# The absolute path to the scripts directory
SCRIPTPATH="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT=$SCRIPTPATH/$PROJECT_ROOT_FROM_SCRIPTS_LOCATION

set-project-root() {
  cd $PROJECT_ROOT
}

run-cluster() {
    docker-compose up
}

destroy-cluster() {
    docker-compose down
}

kafka-generator() {
    cp kafka-generator/target/scala-2.11/kafka-generator-*-with-dependencies.jar shared/kafka-generator/kafka-generator-with-dependencies.jar
    docker exec -it gateway java -cp /shared/kafka-generator:/shared/kafka-generator/kafka-generator-with-dependencies.jar data.processing.kafkagenerator.Generator
}

stream-from-kafka-to-parquet-with-checkpoints() {
    cp spark-jobs/target/scala-2.11/spark-jobs-*-with-dependencies.jar shared/spark-jobs/spark-jobs-with-dependencies.jar
    docker exec -it gateway spark2-submit --master yarn --deploy-mode client --executor-memory 512M --executor-cores 1 --class data.processing.spark.jobs.StreamFromKafkaToParquetWithCheckpoints --driver-class-path /shared/spark-jobs/ /shared/spark-jobs/spark-jobs-with-dependencies.jar
}

execute() {
  case "$1" in
    run-cluster)
       run-cluster
       ;;
    destroy-cluster)
        destroy-cluster
        ;;
    kafka-generator)
        kafka-generator ${@:2}
        ;;
    stream-from-kafka-to-parquet-with-checkpoints)
        stream-from-kafka-to-parquet-with-checkpoints ${@:2}
        ;;
    *)
        echo $"Usage: $0 {run-cluster|destroy-cluster|kafka-generator|stream-from-kafka-to-parquet-with-checkpoints}"
        exit 1
  esac
}

set-project-root
execute "$@"